---
title: "lab 7"
author: "Aisha Lakshman"
date: "3/3/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Packages

```{r}
library(tidyverse)
library(broom)
library(pROC)
library(plotROC)
library(knitr)
```

### Part I: Data Prep and Modeling

# Exercise 1
```{r}
spotify <- read_csv("spotify.csv")
glimpse(spotify)

spotify <- spotify %>% 
  drop_na() %>% 
  mutate(target = as.factor(target),
         key = case_when(
           key == 2 ~ "D",
           key == 3 ~ "D#",
           TRUE ~ "Other"))

glimpse(spotify)

ggplot(data = spotify, aes(x = key, fill = target)) +
  geom_bar(position = "fill") + 
  labs(y = "Proportion", title = "Target Vs Key")
```

The figure above plots the relationship between target and key. About 60% of key D songs have a target value of 1. About 30% of key D# songs have a target value of 1. 50% of key "Other" songs have a target value of 1.   

# Exercise 2
```{r}
model <- glm(target ~ acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence, data = spotify, family = binomial)

tidy(model, conf.int = TRUE, exponentiate = FALSE) %>%
  kable(format="markdown", digits = 5)
```
# Exercise 3
```{r}
model_key <- glm(target ~ acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence + key, data = spotify, family = binomial)

summary(model_key)$aic

summary(model)$aic 
```

By comparing the AIC values for the models with key and without key, we can see that the model with key has a lower AIC value. It is best for a model to have a low AIC value, so working with "model_key" is the right way to go. 

# Exercise 4
```{r}
tidy(model_key, conf.int = TRUE, exponentiate = FALSE) %>%
  kable(format="markdown", digits = 5)
```

For observations of key D# the value for target with increase by e^-1.07319.

### Part II: Checking Assumptions

# Exercise 5
```{r}
final_model <- augment(model_key, type.predict = "response", type.residuals = "deviance")

glimpse(final_model)
```

# Exercise 6
```{r}
arm::binnedplot(x = final_model$.fitted, y = final_model$.resid,
                xlab = "Predicted Probabilities", 
                ylab = "Residuals",
                main = "Binned Residual vs. Predicted Values", 
                col.int = FALSE)
```

# Exercise 7
```{r}
arm::binnedplot(x = final_model$danceability, y = final_model$.resid,
                xlab = "Danceability", 
                ylab = "Residuals",
                main = "Binned Residual vs. Danceability", 
                col.int = FALSE)
```

# Exercise 8
```{r}
final_model %>% 
  mutate(resid_key = if_else(.resid > 1 | .resid < -1, "high Residual", "low Residual")) %>% 
  group_by(key, resid_key) %>% 
  summarise(n = n()) %>%
  kable(format="markdown")
```

# Exercise 9

The plot in exercise 6 does not indicate a linear relationship between the fitted and predictor values. Therefore, I would say that the linearity assumption is not satisfied. 

### Part 3: Model Assessment and Prediction

# Exercise 10

```{r}
roc_plot <- ggplot(final_model, aes(d = as.numeric(target), m = .fitted)) +
  geom_roc(n.cuts = 8, labelround = 3) + 
  geom_abline(intercept = 0) + 
  labs(x = "False Positive Rate: 1 - specificity", 
       y = "True Positive Rate: sensitivity")
roc_plot

calc_auc(roc_plot)$AUC
```

# Exercise 11 
It seems like this model does not effectively differentiate the songs the user likes versus those he doesnâ€™t. The curve shown in my ROC plot is above x = 0, y = 0, representing a random classifier. 

# Exercise 12

I think a threshold of 0.58 would be best. This threshold value optimizes the true positive rate while maintaining a small false positive rate. 

# Exercise 13

```{r}
threshold = 0.58
final_model %>% 
  mutate(prediction = if_else(.fitted < threshold, "0:No","1:Yes")) %>% 
  group_by(target, prediction) %>% 
  summarise(n = n()) %>%
  kable(format="markdown")
```

# Exercise 14

Proportion of true positives (sensitivity) = (512 / 512 + 508) = 0.502
Proportion of false positives (1 - specificity) = (508 / 512 + 508) = 0.498
Misclassification rate = (508 + 161 / 512 + 161 + 508 + 836) = 0.302



